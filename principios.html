---
layout: header
---

<div class="mdl-grid">
    <div class="mdl-cell mdl-cell--2-col"></div>
    <div class="demo-blog__posts mdl-cell mdl-cell--8-col">
      <div class="mdl-card mdl-shadow--4dp mdl-cell mdl-cell--12-col">
        <div class="mdl-card__media mdl-color-text--grey-50" id="historia">
            <div class="mdl-card__title" >
                <h1 class="mdl-card__title-text">Principios de la Fisica</h1>
            </div>
        </div>
       <div class="mdl-color-text--grey-700 mdl-card mdl-card__supporting-text">
            <p>Física, según la real academia española, física significa “Ciencia que estudia las propiedades de la materia y de la energía, considerando tan solo los atributos capaces de medida.</p>

            <p>La física se ocupa de los principios esenciales del universo.  Sobre esta materia, se erigen las otras ciencias: Astronomía, química, biología y geología. La belleza de la física consiste en su simplicidad de sus puntos cardinales y en la manera en que un puñado de conceptos y modelos cambia y hace más grande nuestra visión del universo que nos rodea.</p>

            <p>La física se divide en 6 áreas fundamentales, las cuales son:</p>

            <p>1.  Mecánica clásica: Esta estudia el movimiento de objetos realmente más grandes que los átomos y se mueven con una rapidez mucho más lenta que la de la luz</p>

            <p>2.  Relatividad: Teoríaque se encarga de describir los objetos que me mueven a cualquier rapidez, incluso los quemás se aproximan a la velocidad de  la luz.</p>

            <p>3.  Termodinámica: Trata temas como el calor, el trabajo, la temperatura y el comportamiento estadístico de los sistemas con muchas partículas</p>

            <p>4.  Electromagnetismo: Le corresponde el campo de la electricidad, el magnetismo y los campos electromagnéticos</p>

            <p>5.  Óptica: Se encarga de estudiar el comportamiento de la luz y su interacción con la materia.</p>

            <p>6.  Mecánica cuántica: Varias teorías que analizan y comparten el comportamiento de la materia a nivel submicroscópico con las observacionesmacroscópicas.</p>

            <p>También se puede catalogar como física clásica y física moderna:</p>

            <p>·          Física clásica abarca las áreas de la mecánica clásica, la termodinámica, electromagnetismo y la óptica, las cuales fueron desarrolladas antes de 1900 y su mayor exponente fue Isaac Newton.</p>

             

            <p>·      Física moderna inicia con una gran revolución en esta misma en el siglo XlX  y esta nació debido a que la física clásica no podía explicar algunos fenómenos. Esta abarca las áreas de la Relatividad y la mecánica cuántica. El mayor exponente de esta época fue el científico Albert Einstein.</p>

             

            <p>Sistemas de unidades</p>

            <p>Existen actualmente dos sistemas de unidades en el mundo, el sistema internacional de unidades y el sistema técnico de unidades:</p>

            <p>Sistema internacional de unidades (SI): Este sistema maneja unidades de Longitud, masa y tiempo.</p>

            <p>Sistema Técnico (Tec): El sistema de unidades técnico maneja unidades de Longitud, fuerza y tiempo.</p>

            <p>Ambos emplean los llamados sistemas MKS, CGS E Inglés, a continuación se especificaran sus respectivas unidades.</p>

            <h4>MKS:</h4>

            <p>TIEMPO: Primordial, segundo</p>

            <p>Secundarios, Nanosegundo, Microsegundo, décima de segundo, minuto, hora, día, semana, mes, año.</p>

            <p>MASA: Primordial, Kilogramo</p>

            <p>Secundarios, Miligramo, gramo, tonelada</p>

            <p>LONGITUD: Primordial, Metro</p>

            <p>Secundarias Nanómetro, micrómetro, Milímetro, centímetro, decámetro, hectómetro, kilometro</p>

            <p>FUERZA: Las mismas que la masa</p>

            <h4>CGS:</h4>

            <p>TIEMPO: Primordial, segundo</p>

            <p>Secundarios: Nanosegundo, microsegundo, décima de segundo, minuto, hora, día, semana, mes y año</p>

            <p>MASA: Primordial: Gramo</p>

            <p>Secundarios: Miligramo, kilogramo y tonelada</p>

            <p>LONGITUD: Primordial: Centímetro</p>

            <p>Secundarios: Nanómetro, micrómetro, milímetro, metro, centímetro, decámetro, hectómetro y kilometro</p>

            <p>FUERZA: Mismas que la masa</p>

            <h4>INGLÉS:</h4>

            <p>TIEMPO: Primordial, Segundo</p>

            <p>Secundarios: Nanosegundo, microsegundo, décima de segundo, minuto, hora, día, semana, mes y año</p>

            <p>MASA: Primordial, Libra</p>

            <p>Secundarias: Dracma, Onza, quinta, cuarta y tonelada</p>

            <p>LONGITUD: Primordial, Pulgada</p>

            <p>Secundarias: Pie, yarda, rod, cadena, furlong, milla y legua</p>

            <p>FUERZA, Mismas que la masa</p>

             

            <p>Por el momento solo mencionaremos estas unidades debido a que solo manejaremos como introducción este escrito.</p>

             

             

            <p>Longitud</p>

            <p>La distancia entre dos puntos en el espacio se identifica como longitud, en 1120el rey de Inglaterra establecióque en su país, la medida para utilizar seria la yarda y seria de la longitud de la punta de la nariz hasta el final de su brazo extendido hacia adelante. Después, la medida estándar adoptada por los franceses fue el pie, el cual media exactamente lo que medía el pie del rey Luis XVl. Cabe mencionar que esta medida no fue inventada por los franceses, el pie data del tiempo de los egipcios, los cuales usaban medidas de pulgada, pie y brazo. Retomando el tema, debido a que la longitud del pie de los reyes variaba, optaron por cambiarlo al metro en 1799, el cual era definido como la diezmillonésima parte desde a distancia del ecuador al polo norte a lo largo de una línea que pasa sobre París.</p>

            <p>Tiempo después, en 1960, la definición de la longitud del metro cambió a la distancia entre dos líneas en una cierta barra de platino-iridio almacenada especialmente en Francia.</p>

            <p>10 años después, debido a los requerimientos de la ciencia, el metro pasó a ser una medida más exacta.  Ahora el metro se definió como 1650763.73 longitudes de onda de cierto gas noble. A pesar de la exactitud de esta definición del metro, en 1983, este se redefinió como la distancia recorrida por la luz en el vacío durante un tiempo de 1/299792458 segundos. Esta definición del metro es válida para todo el universo, pues no se basa en materiales terrestres.</p>

             

            <p>Masa</p>

            <p>El kilogramo es la unidad fundamental de la masa en el sistema internacional de unidades y esta es definida como la masa de un cilindro de aleación platino – iridio  que es resguardado en la oficina internacional de pesos y medidas en la ciudad de Sevres, en Francia. La unidad estándar de la masa, la cual es el kilogramos, fu establecida en 1887 y no se ha cambiado el desde 1887 debido a que  platino – iridio es una aleación realmente e inusualmente estable. Existe un duplicado del cilindro de la ciudad de Sevre en el Instituto Nacional de Estándares y Tecnología en Gaithesburg, Maryland, en Estados Unidos.}</p>

             

             

            <p>Tiempo</p>

            <p>Antes del año 1960 la medida estándar del tiempo era definido en términos como día solar medio antes de 1900. Actualmente, la unidad fundamental es el segundo, la cual fue definida como (1/60) (1/60) (1/24) de un día solar medio. Actualmente en nuestros días es conocido que el tiempo de la rotación de la tierra varia ligeramente con el tiempo. Es por eso que este movimiento no proporcione un tiempo estándar que sea constante.</p>

             <p>En el año de 1967 esta unidad, el segundo, fue redefinido para poder aprovechar la ventaja dada por la enorme precisiónque se logra con un dispositivo llamado reloj atómico, que mide vibraciones de los átomos del elemento cesio. Ahora en nuestro días, el segundo de define como 9192631770 veces el periodo de vibración de la radiación del átomo del cesio 133.</p>

             

            <p>Análisis dimensional</p>

            <p>Debido a la especialización de la física, la palabra dimensión toma un significado especial en esta materia. Esta denota la naturaleza física de una cantidad. Ya sea que una distancia se mida en unidades de pies, metro o brazos, todavía es una distancia; se dice que su dimensión es una longitud.</p>

            <p>En muchas situaciones es posible notar que deba verificarse una ecuaciónespecífica, para ver si esta tiene todo lo que buscamos en ella. Existe un proceso llamado análisis dimensional que ayuda a la comprobación de las formulas porque las dimensiones son tratadas como cantidades algebraicas. Por ejemplo, las cantidades se suman o restan solo si tienen las mismas dimensiones. Al seguir estas simples reglas será posible para ti utilizar el análisis dimensional para poder determinar si una expresión tiene la forma adecuada o correcta. Cualquier correspondencia y semejanza es correcta solo si las dimensiones en ambos lados de la ecuación son las mismas.</p>

             <p>Para comprender mejor el concepto, supongamos queestá interesado en una ecuación para la posición x de un automóvil en un tiempo t. Si el automóvil parte del reposo en x=0 y se mueve en aceleración constante.</p>

            <p>A)        La expresión correcta para esta situación es x=1/2at elevado al cuadrado. En este caso, aplicando el análisis dimensional para poder probar la validez de esta fórmula. La cantidad x en el lado izquierdo tiene la dimensión de longitud. Para que la ecuación pueda ser adecuada y correcta en términos dimensionales, La cantidad en el lado derecho también se debe tener la dimensión de longitud. Es posible realizar una verificación dimensional al sustituir las dimensiones para aceleración l/tal cuadrado y tiempo en la ecuación. Esto es en forma dimensional de la ecuación x=1/2at al cuadrado es:</p>

             

            <p>L= (L/T2) t3= L</p>

             

            <p>Las dimensiones de tiempo se cancelan, lo que deja a la dimensión de longitud en el lado derecho para que se pueda igualar con la izquierda.</p>

            <p>Un procedimientomás general de análisis dimensional es establecer la expresión de la forma:</p>

             

            <p>x∞ an tm</p>

             

            <p>Donde ny m son exponentes que se deben determinar y el símbolo ∞ indica una proporcionalidad. Esta correspondencia es correcta solo si las dimensiones de ambos lados son exactamente las mismas. Debido a que la dimensión del lado izquierdo es longitud, la dimensión del lado derecho también debe de ser longitud. Quedaríaasí:</p>

             

            <p>[an tm] = L = L1 T0</p>

             

            <p>Debido a  que las dimensiones  de esta aceleración son L/T2 y la dimensión de tiempo es T:</p>

             

            <p>(L/T2)nTm = L1T0   =      (LnTm-2n) = LT0</p>

             

            <p>Los exponentes de L Y T deben ser los mismos en ambos lados de la ecuación. A partir de los exponentes de L, se nota de manera inmediata que n=. De los exponentes de T,m – 2 n= 0, lo que , una vez que se sustituye para n produce m ¿ 2 . Al regresar a la expresión original x∞ antm se concluye que xα at2.</p>

             

             

             

            <p>Inicios de la física</p>

            <p>Debido a que la física y las matemáticas van casi de la mano al principio, podriamos concluir que la mayoría de las aportaciones de la antigua Grecia aplican a la física y a las matemáticas.</p>

             

            <h4>Termodinámica</h4>

            <p>La termodinámica, esta como disciplina científica se considera que comienza con el científico Otto Von Guericke  quien, durante 1650, diseño y construyó y diseño  la famosa  bomba de vacío y demostró dichas propiedades usando sus hemisferios de Magdeburgo.  En 1656 Robert Boyle estudio y mejoro los diseños de Guericke. En 1679, un asociado de Boyle, Deni Papin apoyándose de los conceptos de Boyle, cosntruyo un digestor de vapor.</p>

            <p>En 1697, el ingeniero Thomas Savery,  a partir de lo hecho por Papin construyó su primer motor térmico, después le siguió Thomas Newcome en 1712, aunque estos motores eran toscos e ineficientes. Durante 1733, Bernoulli uso métodos estadísticos, junto con la mecánica clásica. De 1781 los conceptos de capacidad calorífica y de calor latente, fueron creados por el profesor Joseph Black de la Universidad  de Glasgow. Más tarde en 1783 Lavoiser  propone la teoría del calórico, después de esto en 1798 Benjamin Thompson demostró la conversión del trabajo mecanico en calor.</p>

             <p>Después de todo esto y en base hacia todo, Sadi Carnot ( quien fue llamado el padre de la termodinámica), publico en 1824 un discurso sobre la eficiencia de la termodinámica, la energía, el motor y la energía motriz. El primer libro que se tiene historia de la termodinámica fue escrito en 1859 por William Rankine, después consecutivamente surgieron la Primer y la Segunda Ley de la termodinámica, esto durante la década de 1850, principalmente por Germain Henri, William Rankine, Rudolf Clausius, etc.</p>

            <p>Mas después en este mismo siglo Daniel Bernouli aplica los razonamientos mediante la estadística para explicar el comportamiento de sistemas de fluidos. Durante los años cincuenta del siglo XIX marcaron un record en el estudio de los sitemas térmicos. Durante esos años la termodinámica había crecido básicamente gracias al estudio experimental del comportamiento macroscópico de sistemas físicos, esto a partir de los trabajos de Nicolas Leonard Sadi Carnot James Prescott Joule Clausius y Kelvin, daba a concluir que era una disciplina estable de la física. Las conclusiones que se obtuvieron de las dos primeras leyes coincidían con los resultados de los experimentos realizados. Paralelamente,  la teoría cinética de los gases que se basaba más en la especulación que en los cálculos, comenzó a surgir como una nueva teoría matemática real.</p>

            <p>Pero no fue sino hasta Ludwig  en 1872 creó su teorema H y de esta forma establecería un enlace directo entre la entropía y la dinámica molecular. Exactamente al mismo tiempo, la teoría cinetica empezó a dar a luz a un sofisticado predecesor: la teoría del ensamble.</p>

            <h4>Óptica</h4>

            <p>Además de los avances que se tuvieron dentro de la termodinámica, también ubo otra rama de la física que fue creciendo y haciéndose mas importante dentro de la física, estamos hablando de la Optica.</p>

            <p>Lo que se conoce como la ley de la refracción fue descubierta experimentalmente en 1621 por Willebrord. Mientras que en 1675 Pierre anuncio el principio minimo y a partir de este obtuvo la ley de la refracción.</p>

            <p>Después de esto Robert Boyle y Robert Hooke la mejoraron, y a dicha teoría la propuso Isaac Newton, después los demás fueron descubriendo de forma independiente el fenómeno de la interferencia conocida como anillos de Newton, hooke también los había visto pero este fenómeno ya había sido descubierto por Francesco Grimaldi. Pero Hooke pensaba que esta consisita en una serie de vibraciones propagadas de forma instantánea a gran velocidad y creía que se creaba una esfera que crecía de forma regular. Ya con esta ideas Hooke intento el explicar este fenómeno e interpretar los colores, pero a pesar de esto, Newton que ya había descubierto tiempo antes uqe la luz blanca se dividía en los colores componentes mediante la utilización de un prisma  y encontró que un color puro  se encontraba por una refractabilidad especifica, y debido a problemas que tuvo con otra teoría esta la llevaron a decidirse por la teoría corpusular, que supone que la luz se propaga desde los cuerpos luminosos en forma de partículas. Durante la época que Newton publico su teoría del color, no se conocía la forma en que la luz se propagaba, si de forma instantánea o no. Pero al final el que descubrió la velocidad finita de la luz lo realizo Olaf en 1675 a partir de las observaciones que se hicieron de los eclipses de Júpiter.</p>

            <p>Por otra parte, Hooke fue uno de los primeros que defendieron la teoría ondulatoria que fue expandida y mejorada por Huygens que pronuncio el principio que lleva como tal su nombre, según el cual: “cada punto perturbado por una onda puede considerarse como el centro de una nueva onda secundaria, la envolvente de estas ondas secundarias define el frente de onda de un tiempo posterior”. Gracias a la ayuda de este principio, obtuvo al deducir las leyes de la reflexión y refracción. Además logro interpretar la doble  refracción del espato de Islandia, fenómeno que fue descubierto por Erasmus Bartholinus en 1669, gracias a que suposición de la transmisión de un onda secundaria elipsoidal, también además de la principal forma que tenia, esférica.</p>

            <p>Durante la investigación Huygens descubrió la polarización. Cada uno de los rayos que emergían de la refracción del material, girando alrededor de un eje  con igual dirección que el que tiene le rayo luminoso. Pero a pesar de todo esto fue Newton el que logro interpretar este fenómeno, suponiendo  que los rayos tenían lados, propiedad que le pareció insuperable para la gran teoría ondulatoria de la luz, ya que en aquel entonces los científicos solo estaban familiarizados con las ondas longitudinales.</p>

            <p>La gran fama que Newton tenía en ese entonces, produjo que fuese rechazada por parte de la comunidad científica de la teoría ondulatoria, que durante casi un siglo, claro con algunas excepciones, como la de Euler. Gracias a esto no fue sino hasta el comienzo del siglo XIX en que nuevos avances llevaron a la aceptación general de la teoría ondulatoria. El primero de ellos fue la que anuncio Thomas Young en 1801, del principio de interferencia y la explicación de los colores de las películas delgadas. Pero por la razón de cómo fueron explicadas en términos cualitativos no lograron el reconocimiento general. Durante esta misma época Etienne-Louis describió la polarización por reflexión, que en 1808 observo la reflexión del Sol desde una cornisa a trabes de un cristal de espato de Islandia y encontró que las dos imágenes  laterales variaban sus intensidades relativas al girar el cristal, aunque Malus no intento explicar el fenómeno.</p>

            <p>Fresnel obtuvo un premio instituido en 1818 por la academia de Paris por la explicación de la difracción, esto basándose en la teoría ondulatoria, que la número uno de una colección de investigaciones que, a lo largo de algunos años, termino por des validar completamente la teoría corpuscular. Los principios básicos utilizados fueron: la interferencia de Young y el principio de Huygens, de los cuales, según demostró Fresnel , son suficientes para podes explicar, no solo la propagación rectilínea, sino además las desviaciones de dicho comportamiento. Lo que Fresnel logro fue calcular la difracción causada por rendijas, pantallas y pequeñas aperturas. Algo que fue confirmado experimentalmente de su teoría de la difracción fue el chequeo realizado por Jean y Dominique de una previa predicción de Poisson a través de las teorías de Fresnel, que es la gran existencia de una mancha brillante en el centro de la sombra de un círculo pequeño.</p>

            <p>Durante ese mismo año Fresnel además investigo el problema que se tenía en la influencia del movimiento terrestre en la propagación de la luz. Principalmente el problema era el determinación si existía alguna diferencia entre la luz de las estrellas y la de fuentes terrestres. Arago logro encontrar mediante experimentos que no había diferencia. Sobre esta la base en el descubrimiento Fresnel desarrollo su teoría de la convección parcial  del éter por medio de la interacción con la materia, dando como resultado buenos experimentados en 1851 por Armand. Después junto con Arago, Fresnel investigo la interferencia que había en los rayos polarizados y encontró en 1816 que dos rayos polarizados perpendicularmente uno al otro, nunca se interferirán. Después este hecho no pudo ser reconciliado con la hipótesis de ondas  longitudinales, sino hasta que entonces se había dado por segura. Luego Young explico en 1817 el fenómeno con la suposición de ondas transversales de ondas transversales.</p>

            <p>Después de esto Fernel intento el explicar la propagación de la luz como ondas en un material (en este caso éter) y debido a que en un fluido solo son posibles las oscilaciones longitudinales elásticas, este concluyó que el éter debía comportarse como si fuese un sólido, pero como en aquel entonces la teoría de de las ondas elásticas en sólidos aun no estaba desarrollada, lo que Fresnel intento fue el deducir las propiedades del éter esto de la observación experimental. El punto de partida fueron las leyes de la propagación en los cristales. Para entonces en 1832, William predijo a partir de las ya mencionadas teorías de Fresnel la denominada refracción cónica, que fue confirmada posteriormente de forma experimental por Humprey.</p>

            <p>Además fue Fresnel quien el año de 1821 dio la primera indicación de lo que causaba de separación al considerar la estructura molecular de la materia, idea que fue desarrollada posteriormente por Cauchy.</p>

            <p>Los denominados modelos dinámicos de los mecanismos de las vibraciones del éter, condujeron a Fresnel a deducir las nuevas leye que ahora llevan su nombre y que aun gobiernan la intensidad  y la polarización de los rayos luminosos reproducidos por la reflexión y refracción.</p>

            <p>Ya por alla de 1850 Foucalt, Fizeau y Breguet, quienes realizaron un experimento que fue crucial para decidir entre la teoría corpuscular y ondulatoria. Este experimento fue propuesto inicialmente por Argo y este consistía en medir la velocidad de la luz en aire y en el agua. En la teoría corpuscular explica la refracción en términos de la atracción de los corpúsculos luminosos hacia el medio más denso posible, lo que esto implica una mayor velocidad en el medio que sea más denso. Pero por otra parte, la teoría ondulatoria implica que, de acuerdo con el principio de Huygens  que en el medio más denso posible la velocidad es menor.</p>

            <p>Durante las siguientes décadas,  se desarrollo la teoría del éter. El primer paso que se realizo fue el de formular  una teoría de la elasticidad de los cuerpos sólidos que fue desarrollada por Claude, Louis, Henri, que considero que la materia que es consistente de un conjunto de partículas aplicando entre ellas fuerzas a lo largo de las líneas que unen a estas. Diferentes fueron los desarrollos aplicables en la Opticafueron realizados por Simeon, George Green y Franz Neuman. Todos ellos encontraron que habían dificultades para el poder explicar el fenómeno óptico en términos mecánicos. Un ejemplo de esto es que , al incluir sobre un medio una onda transversal , estos se deberían producir, tanto longitudinales como transversales, pero, según los experimentos echos por Arago y Fresnel, solo se producen del segundo tipo. Otra refutación a la hipótesis del éter es la ausencia de resistencia al movimiento de los planetas.</p>

            <p>Uno de los primeros pasos para desertar el concepto de éter elástico lo hizo MacCullagh, quien postulo un medio con propiedades diferentes a las que los cuerpos ordinarios tenían. Estas leyes de propagación de ondas en este tipo de éter son parecidos a las ecuaciones electromagnéticas de Maxwell.</p>

            <p>A pesar de todas las dificultades, la teoría del éter elástico persistió y recibió grandes aportaciones de físicos de los siglos XIX, entre ellos William Thomsom, John William y Gustav kirchhoff.</p>

            Mientras ocurria todo esto, las investigaciones en magnetismo y electricidad  se siguieron desarrollando y culminando en los descubrimientos  hechos por Michael Faraday. Después de esto James Maxwell  resumir todo el conocimiento previo en este campo mediante un sistema de ecuaciones que establecían la posibilidad de ondas magnéticas con una velocidad que se podía calcular a partir de los resultados de medidas eléctricas y magnéticas. Ya cuando Rudolph y Wilhem realizaron dichas mediciones, la velocidad obtenida resulto coincidir con la velocidad de la luz. Todo esto llevo a que Maxwell llegara a especular que las ondas luminosas eran electromagnéticas, lo que luego se verifico experimentalmente en 1888 por Heinrich.</p>

            <p>La Física en el Siglo XIX</p>

            <p>Durante el siglo XIX en la física, todas las ramas de esta pero en especial alguna fueron incrementando y siendo aun mas importante dentro de lo que cabe en la vida cotidiana pues se fue descubriendo como generar la energía eléctrica y el cómo esta era transferida o pasado por los distinto medios de creación y propagación. A continuación se explicaran varios de los inventos y de las leyes y teoría que fueron creadas durante este siglo en especial.</p>

            <p>En los años 1800, Alessandro Volta construyo el primero de los dispositivos para producir una corriente eléctrica grande,  posterior a esto conocido como la batería eléctrica. Napoleon sabiendo de sus trabajos, en 1801 para que hiciera una demostración de sus experimentos. Gracias a esto recibió una gran cantidad de medallas y condecoraciones, también incluida la Legión de honor.</p>

            <p>Más tarde Davy en 1806, utilizando una pila voltaica de un aproximado de 250 células, o parejas, descompuso sosa y potasa, demostrando que estas sustancias eras, respectivamente, los óxidos de potasio y sodio, cuyos metales eran conocidos más bien hasta entonces. Estos experimentos fueron el gran comienzo de lo que hoy se conoce como electroquímica, la investigación que tomo Faraday y sobre la cual en 1833 anuncio du muy importante ley  de los equivalentes electroquímicos, es decir : “La misma cantidad de electricidad, es decir, la misma corriente eléctrica, descompone químicamente cantidades equivalentes de todos los cuerpos que atraviesa, de ahí los pesos de los elementos que están separados en estos electrolitos estén relacionados unos con otros como sus equivalentes químicos”.</p>

            <p>Lo que fue el descubrimiento de la inducción electromagnética se hizo al mismo tiempo, aunque de una forma independiente, esto por   Michael Faraday y Joseph Henry. Mientras que los primeros resultados obtenidos por Faraday sucedieron a los de Henry, Henry fue quien primero uso el principio del transformador. Este descubrimiento que logro Henry de la autoinducción y además su trabajo en conductores de espiral utilizaron una bobina de cobre se hicieron  de carácter publico en 1835, esto justo antes de las de Faraday.</p>

            <p>Después en 1831 comenzaron las investigaciones de Faraday, quien fue el famosos discípulo y sucesor del gran Humphry a la cabeza de la gran Royal Institution de Londres, esto en relación con la inducción electromagnética. Gracias a esto los estudios e investigaciones de Faraday se extendieron desde 1831 hasta 1855 y una descripción detallada de sus experimentos hechos así como deducciones y especulaciones se encuentra en su publicación, titulada “Investigaciones Experimentales en Electricidad”. Fraday, quien era químico de profesión. No estaba en una posesión del más remoto titulo en las matemáticas hablando en el sentido ordinario, de hecho si se juntan todos sus escritos se darán cuenta de que hay solo una formula matemáticas en estos.</p>

            <p>En lo que cabe mencionar los experimentos hechos por  Faraday lo condujeron al descubrimiento de la inducción  se realizo lo siguiente: se hizo un estructura lo que es ahora y entonces se domino bobina de inducción, cuyos conductores del primario y secundario se envolvieron en una bobina de madera, una lado del otro y aislados entre ellos. Dentro del circuito del cable principal se coloco una batería de un aproximado de 100 celdas. Mientras en el cable del secundario se le incorporo un galvanómetro. Al de realizar su primer prueba  no observó ningún resultado, el galvanómetro se mantenía en reposo, pero al incrementar  la longitud de los conductores se dio cuenta de una desviación que ocurría en el galvanómetro en el conductor de la secundaria cuando el circuito del conductor principal se cerraba y abría. Esta es la primera observación del desarrollo de la fuerza electromotriz por el uso de inducción electromagnética.</p>

            <p>Además también descubrió que aparecían corrientes  inducidas  el de un segundo circuito cerrado en cuando a la intensidad de la corriente variaba fuertemente en el primer conductor,  y que la dirección en que la corriente en el circuito secundario es contraria a la del primer circuito. Igualmente que en una se induce corriente inducida en un circuito secundario cuando otro circuito por cual circula una corriente que se mueve hacia y desde el primer circuito, y que la aproximación o el alejamiento de un imán o de algún circuito cerrado induce unas corrientes momentáneamente en el segundo. En total, en el espacio de unos pocos meses Faraday logro descubrir mediante una serie de experimentos prácticamente todas las leyes y hechos que actualmente son conocidos sobre la inducción electromagnética y la inducción magnetoeléctrica.</p>

            <p>Estos descubrimientos , casi sin ninguna excepción, depende el que funcione el teléfono, el dinamos, y los relacionados con el dinamos, prácticamente estamos hablando de todas las industrias eléctricas más grandes del mundo, también incluyendo la luz eléctrica, la tracción eléctrica, el funcionamiento de los motores eléctricos para producir potencia, y la galvanoplastia, etc.</p>

            <p>Además en sus investigaciones realizadas de la manera más cuidadosa en que las limaduras de hierro se disponen sobre un cartón o vidrio en las cercanías de los polos de un imán, por ende Faraday, tuvo la idea de “líneas de fuerza” magnéticas que se extienden de polo a polo desde el imán y a lo  largo de las cuales las limadura se tienden a situar. Durante el descubrimiento realizado de los efectos magnéticos acompañan a el paso de una corriente eléctrica en un conductor, además de suponer que las líneas similares de fuerza magnética giran alrededor del alambre. Además por comodidad y para dar cuenta de la electricidad inducida se asumió que cuando esta líneas de fuerza son “cortadas” por causa de conductor  que pasa a través de ellas o cuando estas líneas de fuerza en la apertura y el cierre de algún circuito cortan conductor, se logra una corriente eléctrica, o más bien para ser más exactos, se desarrolla una gran fuerza electromotriz en el conductor que establece una corriente en un circuito cerrado.</p>

            <p>Faraday logro avanzar a lo que se ha denominado “teoría molecular de la electricidad”, que da a entender que la electricidad es la manifestación de un estado particular de l.as moléculas del cuerpo frotado o del éter rodea el cuerpo. Además Faraday, mediante experimentos, logro descubrir el paramagnetismo y el diamagnetismo, dando a saber que todos los sólidos y todos los líquidos son repelidos o atraídos por un imán.</p>

            <p>Durante 1778 Brugans de Leiden y en 1827 Becquerel habían descubierto el diamagnetismo, esto en el caso del bismuto y el antimonio. Además Faraday también descubrió la capacidad inductiva específica 1837, cuyos resultados los experimentos de Cavendish no habían publicados en  ese entonces no habían sido publicadas en esa época. También predijo el retraso de las señales en los largos cables submarinos debido al efecto inductivo del asilamiento del cable, en otras palabras, la capacidad estática del cable. Durante los 25 años inmediatamente  después del descubrimiento de Faraday de la inducción eléctrica cual fue muy satisfactorio en la promulgación de hechos relativos a las corrientes inducidas y el magnetismo.</p>

            <p>Ya en 1834, Jacobi y Lenz  de forma independiente demostraron el hecho actualmente familiar  de lo que es la corriente inducida  dentro de una bobina es proporcional al número vueltas  en la bobina. Lenz además anuncio en ese momento la ley importante que lleva su nombre, de que en todos los  casos de este de inducción electromagnética, en donde las corrientes inducidas tienen en una  dirección tal que su reacción tiende a detener el movimiento que lo produce, ellos, una ley que tal vez, y tolo tal vez  era deducible de la gran explicación de Faraday de las rotaciones de Argo.</p>

            <p>Ya en 1845, Joseph Henry, el físico estadounidense, publico un relato de sus valiosas e interesantes experiencias, que gracias a esta muestra que las corrientes de orden superior pueden ser  inducidas a partir del segundo de una bobina de inducción al primario de una segunda bobina, de allí a su conductor secundario, y así sucesivamente hasta el primer de una tercera bobina, etc.</p>

            <p>Después más tarde la teoría conocida como electromagnética de la luz añade a la antigua teoría ondulatoria un enorme interés e importancia: esta nos exige no solo una explicación de todos los fenómenos completos de la luz y también de los del calor radiante mediante ondas transversales de un medio elástico solido llamado éter, sino también  a la conclusión de las corrientes eléctricas y además del magnetismo permanente del acero y del imán, de la fuerza electroestática  y de la fuerza magnética, en una gran teoría del éter.</p>

            <p>No fue sino hasta mediados del siglo XIX, muy de cerca de 1870, la ciencia eléctrica fue, asi se puede decir, un libro cerrado  para la mayoría, si no es que todo, de los investigadores eléctricos. Pues antes de esta época, una serie de manuales que se publicaron sobre la electricidad y el magnetismo, muy en particular, el exhaustivo Tratado de electricidad de Auguste Arthur.</p>

            <p>Hacia los años 1850 Gustav publico sus leyes relativas a las ramas o los círculos divididos. Además  también demostró mediante las matemáticas que, según la teoría electrodinámica vigente en ese entonces, la electricidad se propaga a lo largo de un cable perfectamente  conductor con la velocidad de la luz. Helmholtz  investigo mediante las matemáticas de los efectos que ocurrían de la inducción sobre la fuerza corriente  y así logro deducir ecuaciones, que en los experimentos confirmaron que demuestra entre otros puntos importantes el efecto retardador de la autoinducción en específicas condiciones del circuito.</p>

            <p>Tres años después de esto Sir Thomson predice como resultado de los cálculos matemáticos la naturaleza oscilatoria de la descarga eléctrica de un círculo condensador. Pero a Henry sin embargo, pertenece el merito de demostrar, como un resultado de sus experimentos  el año de 1842, el carácter oscilatorio de la descarga de la botella de Leyden. Lo que el realizo o más bien escribió: “Los  fenómenos nos obligan a admitir la existencia  de una descarga principal en una dirección, y después varias acciones de reflejadas hacia atrás y hacia adelante, cada una mas débil que la anterior, hasta que se obtiene el equilibrio”.</p>

            <p>Ya hacia 1876 el Prof.  H. A. Rowland de Baltimore de mostro lo importan que era el hecho de que una carga estática que gira produce los mismos  efectos de magnetismo que en una corriente eléctrica. La importancia que este tiene este descubrimiento consiste en que puede ofrecer una teoría razonable del magnetismo, es otra palabra, que el magnetismo puede ser el resultado del movimiento de las filas de moléculas que transportan cargas estáticas.</p>

             

            <p>Después del descubrimiento de Faraday de que las corrientes eléctricas podrían desarrollarse en un conductor, al cortar el de conductor las líneas de fuerza de un imán, era de esperar que se emprendiera la construcción de máquinas que aprovecharan este hecho para el desarrollo de corrientes voltaicas.</p>

             

            <p>Un avance notable en el arte de la construcción de dinamos fue hecha por el Sr. S.A. Varley en 1866 y por el Dr. Charles William Siemens y el Sr. Charles Wheatstone que de forma independiente descubrieron que cuando una bobina de un conductor, o una armadura, de la máquina dinamo se hace girar entre los polos (o en el «campo») de un electroimán, aparece una débil corriente en la bobina debido al magnetismo residual en el hierro del electroimán, y que si el circuito de la armadura se conecta con el circuito del electroimán, la débil corriente desarrollada en la armadura aumenta el magnetismo en el campo.</p>

            <p>En 1860, fue realizada una mejora importante por el Dr. Antonio Pacinotti de Pisa, quien ideó la primera máquina eléctrica con una armadura de anillo. Esta máquina fue utilizada por primera vez como de un motor eléctrico, pero después, como un generador de electricidad. En 1872, Heffner -Altneck idearon el tambor de la armadura. Esta máquina en una forma modificada posteriormente fue conocida como la dinamo Siemens.</p>

            <p>Desde que empezaron a funcionar alrededor de 1887 los generadores de corriente alterna tuvieron de una extensa utilización y un amplio desarrollo comercial del transformador, mediante el cual las corrientes de bajo voltaje y alta intensidad de la corriente se transformaban en corrientes de alta tensión y baja intensidad de corriente, y viceversa, lo que en su tiempo de revolucionó la transmisión de energía eléctrica y a largas distancias. Asimismo, la introducción del convertidor rotatorio que convierte la corriente alterna en corrientes de continuas (y viceversa) ha efectuado grandes economías no en el funcionamiento de los sistemas eléctricos. Ver eléctrica alterna maquinaria actual diarreico.</p>

            <p>En 1864, James Clerk Maxwell de Edimburgo, anunció su teoría electromagnética de la luz, que fue quizás el paso más grande de en el conocimiento del mundo de la electricidad. Maxwell había estudiado y comentado en el ámbito de la electricidad y el magnetismo tan pronto como en 1855-56, desde el de cuando fue leída 'On Faraday's lines of force', en la Cambridge Philosophical Society. En el documento presenta un modelo simplificado de trabajo de Faraday, y cómo estaban relacionados los dos fenómenos. Redujo todo el conocimiento actual en un conjunto enlazado de ecuaciones diferenciales con 20 ecuaciones con 20 variables. Este trabajo fue publicado posteriormente como On Physical Lines of Force en marzo de 1861.</p>

            <p>Como ya se dijo aquí Faraday, y antes de él, Ampere y otros, había atisbos de que el éter lumínico del espacio es también y el medio para la acción eléctrica. Aproximadamente 300.000 kilómetros por segundo, es decir, igual a la velocidad de la luz, lo que en sí mismo sugiere la idea de una relación entre la electricidad y la «luz».</p>

             

            <p>Maxwell amplió este punto de vista las corrientes de desplazamiento en los dieléctricos al éter del espacio. Suponiendo que la luz sea la manifestación de alteraciones de las corrientes eléctricas en el éter, y vibrando al ritmo de las vibraciones de la luz, estas vibraciones por inducción crean las correspondientes vibraciones en porciones adyacentes del éter, y de esta manera las ondulaciones que corresponden a las de la luz propagan como un efecto electromagnético en el éter. La teoría electromagnética de Maxwell de la luz, obviamente implicaba la existencia de ondas eléctricas en el espacio, y sus seguidores se dedicaron a la tarea de demostrar experimentalmente la veracidad de la teoría.</p>

            <p>En 1891, se realizaron notables aportaciones a nuestro conocimiento de los fenómenos electromagnéticos a alta frecuencia y alto potencial por Nikola Tesla. Entre los nuevos experimentos realizados por Tesla, uno de ellos fue a tomar en su mano un tubo de vidrio del que se había extraído aire, y posteriormente poner su cuerpo en contacto con un conductor que transporte corriente de alto potencial, el tubo se bañó con una luz brillante agradable.</p>

             

            <h4>LA ERA ATOMICA Y EL DESARROLLO DE LA FISICA DEL SIGLO XX</h4>

            <h4>CONTEXTO MUNDIAL Y ALGUNOS MITOS EN EL AMBITO DE LAS MATEMATICAS</h4>

            <p>El siglo XX  traería consigo dos grandes guerras mundiales que paradójicamente darían un impulso al desarrollo del conocimiento científico en aquellas áreas en que se advertían necesidades internas y principalmente con fines relacionados con la tecnología militar. Este desarrollo dio lugar, incluso, al gran problema nuclear de la década de los años cuarenta.</p>

             

            <p>El progreso de las ciencias debió navegar en medio de tales circunstancias históricas. Desde inicios del siglo  comenzó a manifestarse la principal característica de su desarrollo consistente en la transformación, de producto social, elemento de la superestructura de la sociedad humana, en una fuerza productiva con rasgos muy destacados. Esta característica estuvo precedida por una explosión en el ritmo de la producción de los conocimientos científicos que alcanzó un crecimiento extremo. Las relaciones Ciencia – Sociedad se hicieron más complicadas.</p>

             

            <p>Un proceso de fortalecimiento de los vínculos  en la comunidad científica, que se habían iniciado con las Sociedades  creadas en el siglo XVIII, se advierte desde los comienzos del siglo, sufriendo en los períodos de duración de ambas guerras un inevitable debilitamiento. En este contexto se destacan los Congresos realizados en Bruselas, con el apoyo financiero del químico industrial belga Ernest Solvay (1838-1922), que juntaron a los más brillantes físicos de la época.</p>

             

            <p>El Congreso  Solvay de 1911 inaugura el reconocimiento de la comunidad científica a las ideas de la Teoría Cuántica, verdadera revolución en el campo de las Ciencias Físicas. En el período del evento se arribó a un acuerdo de que la Física de Newton y Maxwell si bien explicaba satisfactoriamente los fenómenos macroscópicos era incapaz de interpretar los fenómenos de la interacción de la radiación con la sustancia, o las consecuencias de los movimientos microscópicos de los átomos en las propiedades macroscópicas. Para cumplir este último propósito era necesario recurrir a las ideas de la cuantificación. Eso  demostraba la comprensión de la vanguardia de las Ciencias sobre el carácter temporal, histórico en la construcción del conocimiento científico.</p>

            <p>El siglo XX traería también una organización de la ciencia en Instituciones que debían concentrar sus esfuerzos bien en estudios fundamentales como en aquellos de orden práctico. Los políticos se darían cuenta, desde la Primera Guerra Mundial,  de la importancia de costear los gastos de aquellas investigaciones relacionadas con la tecnología militar.</p>

             

            <p>El Laboratorio Cavendish en Cambridge, fundado en el siglo XIX, hizo época no sólo por la relevancia de sus investigaciones fundamentales para la determinación de la estructura atómica, sino por la excelencia mostrada por sus directores científicos, Joseph John Thompson (1856 – 1940) y Ernest Rutherford (1872 – 1937), que consiguieron  con su liderazgo la reproducción de los valores de la producción científica.</p>

             

            <p>En las primeras décadas del siglo el Laboratorio "Káiser Guillermo" de Berlín se levanto en modelo de institución investigativa y en el período de la Primera Guerra Mundial contó con la asistencia de los más célebres científicos alemanes vinculados a proyectos de desarrollo de nuevas armas. Fritz Haber, destacable  químico alemán jugó el triste papel de introductor del arma química en los campos de batalla.</p>

             

            <p>En la década de los  40, se crea en México, el Laboratorio Nacional de los Álamos, gran empresa científica multinacional, con el objetivo de dar cumplimiento al llamado Proyecto Manhattan para la creación  de la bomba atómica. La movilización de hombres de ciencias de todas las naciones  tuvo el propósito de neutralizar cualquier tentativa de  Alemania hitleriana de emplear la extorción nuclear. El propio Einstein, con su enorme prestigio y autoridad moral, inicia el movimiento enviando una  carta  al presidente de los Estados Unidos. Cinco años después, enterado de los éxitos ya obtenidos en los ensayos de la bomba atómica, vuelve a usar la pluma está vez para reclamar el juicio  en el empleo de este engendro de la Física Nuclear.</p>

            <p>El  9 de agosto de 1945 la humanidad se aterrorizaba con la hecatombe nuclear en Hiroshima,  días después se repetía la escena esta vez en Nagasaki. Se creaba entonces  la época del arma nuclear con un saldo inmediato en Hiroshima de unos 140 mil víctimas  de una población estimada en 350 mil habitantes, y una multiplicación a largo plazo de las víctimas como resultado de las manifestaciones cancerígenas y las mutaciones genéticas inducidas por la radiación nuclear.</p>

             

            <p>Los más relevantes exponentes, y la mayoría de la comunidad científica reaccionaron vigorosamente contra el desarrollo del armamento nuclear y abrazó la causa del uso pacífico de la energía nuclear. Poco antes del lanzamiento de la bomba en Hiroshima, como expresión de las ideas de los científicos que trabajaban en el proyecto Manhattan, 68 científicos que participaron en las investigaciones desarrolladas en el Laboratorio de Metalurgia de la Universidad de Chicago firmaron una carta de petición al presidente de los E.U. para impedir el empleo del arma nuclear. El propio Albert Einstein abogó por el desarme internacional y la creación de un gobierno mundial. No faltaron, sin embargo aquellos que como el físico húngaro, nacionalizado estadounidense, Edward Teller , arquitecto principal de la bomba H, consideraron oportuno continuar la espiral armamentista, confiados en que el liderazgo de un país podía resultar ventajoso para todo el mundo.</p>

             

            <p>Al finalizar la Segunda Guerra Mundial fueron creados  dos grandes bloques militares, económicos y políticos, que se enfrascaron en una guerra fría, desarrollaron una irracional guerra  armamentista, y fomentaron el desarrollo  de un complejo militar industrial. El ruido de la guerra ha tenido una huella que no ha sido evaluada con suficiente precisión. De acuerdo con los resultados del primer "ensayo nuclear" en las Islas Marshall, las superpotencias llevaron sus polígonos de prueba para sitios protegidos por extensas zonas desérticas, e incluso llevaron las pruebas al nivel del subsuelo evitando la contaminación atmosférica y las lluvias radiactivas que suelen llevar impulsados por los vientos, residuos radiactivos a miles de kilómetros del lugar de la explosión. De cualquier manera  el planeta ha sufrido la sacudida tectónica, y el pulso electromagnético de radiaciones ionizantes  provocados por más de dos mil explosiones nucleares, más de la mitad lanzadas por los Estados Unidos, el 85 % por las dos grandes superpotencias del siglo (E.U. y la URSS), el 10% en el orden Francia, China y Gran Bretaña. Cinco pruebas se reparten entre dos países asiáticos envueltos en un pleito histórico: Paquistán y la India. Una bomba atómica fue lanzada en el océano Índico por el régimen sudafricano del apartheid en 1979, para emplear la coacción en sus relaciones con los vecinos africanos. Desde 1992, logrado un Tratado Internacional de no proliferación del arma nuclear, se han silenciado notablemente "los ensayos nucleares". Pero India y Paquistán desobedeciendo el clamor universal, en 1998 realizaron un par de pruebas por parte en demostración mutua de fuerza. Al grupo de ocho países responsables de esta demencial práctica  se ha sumado recientemente Corea del Norte.</p>

             

            <p>La rivalidad dominante este – oeste del siglo se reflejó también entre las instituciones científicas hasta bien avanzado el siglo. A la competencia y el  intercambio que alentó, en lo fundamental, el desarrollo de las investigaciones en las primeras décadas entre las Escuelas de Copenhague, Berlín,  París, y Londres, le sustituyó un cerrado silencio. El intercambio fue tapiado y supuestas filtraciones al bando opuesto adquirieron  la dramática connotación de espionaje político. Los logros publicables que obtenían los laboratorios nucleares de Dubna, en la ex - Unión Soviética,  Darmstad en Alemania,  y Berkeley de los Estados Unidos eran sometidos a encendidas polémicas sobre prioridad, como es el caso del descubrimiento (acaso sería mejor decir "la fabricación" en los aceleradores lineales) de los elementos transférmicos que ocupan una posición  en la tabla periódica posterior al elemento número 100.</p>

             

            <p>El proceso de descolonización en África y Asia experimentó una aceleración en el período de la posguerra. Pero el cuadro del desarrollo socioeconómico de los países a lo largo del siglo se mantuvo tan desigual y asimétrico como irracional resultaría la distribución de riquezas heredada del pasado colonial. La brecha entre ricos y pobres continuó ampliándose y se reflejó necesariamente en el estado de la ciencia y la técnica. Los países "en vías de desarrollo" debieron sufrir otro fenómeno: la fuga de cerebros. El capital humano, tal vez el mayor capital que atesora un país, se ve tentado en los países en desarrollo por las oportunidades que ofrecen las Mecas contemporáneas de las ciencias y al triste fenómeno de la emigración selectiva asisten sin posible defensa ante  el mercado de la inteligencia, los países pobres.</p>

             

            <p>Un panorama similar se advierte si se recurre a cifras que ilustren el financiamiento por países en el área de investigación y desarrollo, así como si se analizan la producción de patentes de invención. En esta última esfera un nuevo problema viene a matizar el progreso científico.</p>

             

            <p>La protección de la propiedad industrial en todo el siglo XIX operó como un elemento de financiamiento de nuevas investigaciones que alentaran y permitieran nuevos logros en la invención. Pero con el siglo XX se van haciendo borrosos los contornos de los descubrimientos y las invenciones para la pupila de las grandes transnacionales interesadas más que todo en competir con éxito en el templo del mercado. Una encendida polémica se viene gestando en la opinión pública que gana creciente conciencia de los peligros que entraña semejante política. Afortunadamente, entre los propios investigadores se desarrolla un movimiento tendiente a preservar como patrimonio de toda la humanidad  los descubrimientos científicos de mayor trascendencia.</p>

            <p>Ya a finales de la década de los años ochenta, con el derrumbe del sistema socialista en el este europeo, se establecieron las bases de un mundo unipolar, caracterizado por un proceso de globalización, que si en principio pudiera considerarse en bien del intercambio científico, potencialmente representa un desafío para la supervivencia del mosaico de culturas de las naciones emergentes y de sus identidades nacionales.</p>

             

            <p>Por otra parte, la desaparición de la guerra fría  y el clima de universal entendimiento que parecía poder alcanzarse brindaban la posibilidad de congelar la irracional carrera de armamentos y desviar estos enormes recursos financieros hacia la esfera del desarrollo. Esto equivale a decir que podría al fin inaugurarse la era en que Ciencia y Tecnología alinearan sus fuerzas en bien de toda la humanidad. Pronto el optimismo inicial, derivado de semejante razonamiento se evaporó ante las nuevas realidades.</p>

             

            <p>En el ámbito de las Matemáticas el siglo se inicia con el Congreso Internacional de París que concentró a las más relevantes figuras del momento y tuvo la significación de contar con las predicciones de David Hilbert (1862 -1943), notable matemático de la célebre Universidad de Gotinga forja académica de Gauss y Riemann y uno de las instituciones dónde se generó la actual interpretación de la Mecánica Cuántica, sobre los problemas más candentes que deberían ser resueltos por el esfuerzo de la comunidad de matemáticos. En efecto, a lo largo del siglo estos problemas serían abordados, pero lo que no pudo Hilbert pronosticar fue que las más significativas aportaciones en las Matemáticas guardarían relación con el mundo de la informatización y la inteligencia artificial. Así aparecen una nueva rama de la Geometría, esta vez la Geometría de los fractales, una nueva Lógica,  la llamada Lógica Difusa, un Álgebra de nuevo tipo, conocida como el Álgebra de Neumann, y una teoría que había sido relegada por la complejidad inherente a su abordaje, la Teoría de los Sistemas Caóticos.</p>

             

            <p>En el año 1946 se construye en Estados Unidos el primer ordenador electrónico digital de uso práctico (enicak), sin pieza mecánica alguna. Desde entonces estos artefactos han tenido un vertiginoso desarrollo, alcanzando su cima en la inteligencia artificial.</p>  

             

            <p>La teoría de los juegos se presenta a fines de los años 20, fomentada principalmente por John von Neumann (1903-1957), matemático estadounidense nacido en Hungría. La teoría de juegos se aplicó a los negocios y las guerras y Neumann extiende sus conceptos para desarrollar nuevos operadores y sistemas conocidos como anillos de operadores que admiten el nombre de Álgebra de Neumann que resultan muy beneficioso en la Mecánica Cuántica.</p>

            <p>A pesar de los grandes adelantos  en la optimización computacional ocurridos durante los últimos 20 años,  el método Simplex inventado por George B. Dantzig (1914-2005) en 1947 es aún la herramienta principal en casi todas las aplicaciones de la Programación Lineal. Las contribuciones de Dantzig abarcan además la teoría  de la descomposición, el análisis de sensibilidad, los métodos de pivotes complementarios, la optimización en gran escala, la programación no lineal, y la programación bajo incertidumbre. De ahí en adelante la Teoría del Caos,  y la Lógica Difusa vienen emergiendo con gran fuerza en el panorama científico y tecnológico.</p>

             

            <p>La Teoría del Caos se ocupa de sistemas que muestran un comportamiento impredecible y aparentemente aleatorio, aunque sus componentes estén regidas por leyes deterministas. Desde 1970 se viene aplicando esta teoría en la esfera de los fenómenos meteorológicos y en la Física Cuántica entre otras, siendo el físico estadounidense Mitchell Feigenbaum (1944- ) uno de los exponentes más representativos. Estos sistemas tienen afinidades con la Geometría Fractal y con la teoría de catástrofes.</p>

             

            <p>La Geometría Fractal fue hallada en la década de los setenta por el matemático polaco, nacionalizado francés, Benoit B. Mandelbrot (1924 - ). Ya no se limita la Geometría a una, dos o tres dimensiones, sino que se plantea el trabajo con dimensiones fraccionarias. Las montañas, nubes, rocas de agregación y galaxias se pueden estudiar como fractales. Estos vienen siendo usados en gráficos por computadora y para hacer más pequeño el tamaño de fotografías e imágenes de vídeo.</p>

             

            <p>La Lógica Difusa fue aplicada en 1965 por Lotfi Zadeh (1921- ), maestro de la universidad de Berkeley. La gran desigualdad con la teoría de conjuntos clásica, enunciada por el alemán George Cantor a finales del siglo XIX, es que un elemento puede pertenecer parcialmente a un conjunto; en contradicción con la concepción tradicional que solo da dos posibilidades: "se pertenece o no se pertenece". La Lógica Difusa conforma una de las técnicas que sustentan la inteligencia artificial y se viene aplicando en Medicina y Biología, Ecología, Economía y Controles Automáticos.</p>

             

            <p>Las Matemáticas irrumpen en el siglo XX  todas las esferas de la sociedad, de la técnica y la ciencia,  y sus más destacadas aportaciones se vinculan con las nuevas áreas de la informatización y la inteligencia artificial. La modelación matemática reina en los procesos de ingeniería, de control automático, de la robótica, se introduce en los procesos biológicos y hasta algunos lo han evocado, a nuestro juicio con excesivo entusiasmo, en la solución de complejos problemas sociales.</p>

            <p>La Revolución en el campo de la Física se abrió camino en el siglo XX a través de la superación de profundas crisis en el campo de las ideas, que exigieron lo que se ha dado en nombrar un cambio de paradigma. La construcción en paralelo de las teorías que pretendían aclarar el universo de las micropartículas y ofrecer una nueva visión del mundo macroscópico, en lugar de hallar un punto convergente que se separan desde sus propios enfoques de partida.</p>

             

            <p>La desintegración radiactiva y la teoría del átomo nuclear.</p>

             

            <p>En seguida se intentara llevar a cabo un breve recorrido por algunos descubrimientos trascendentes de la estructura nuclear del átomo. Al hacerlo daremos a conocer el protagonismo de seres humanos de ciencias e instituciones élites en instantes cruciales vividos por la humanidad, asistiendo a problemas de género, riesgos de subsistencia, deberes políticos, y en fin al drama de las opiniones que los acompañó.</p>

             

            <p>Casi desde estos primeros instantes empezaron las tentativas por detallar un modelo atómico. J.J. Thompson concibe inicialmente la carga positiva colocada uniformemente por todo el átomo mientras los electrones en número que recompensaba esta carga se localizan en el interno de esta nube positiva. Un año más tarde, supone a los electrones en desplazamiento de tipo oscilatorio cerca de de ciertas posiciones de equilibrio adentro de la carga positiva distribuida en una esfera.</p>

             

            <p>Luego de otros intentos para detallar un modelo atómico que aclarara el espectro de rayas y de bandas y el fenómeno o anormalidad de la radioactividad, se presenta en 1911 la publicación del físico neozelandés Ernest Rutherford (1872 – 1937) "La dispersión por parte de la materia, de las partículas alfa y beta, y la estructura del átomo" en la que siguiere el modelo nuclear del átomo. Según Rutherford la carga positiva y prácticamente la masa del átomo se confinan en una ración muy reducida, 104 veces más pequeñas que las dimensiones del átomo, mientras los electrones quedan alojados en una cubierta extra nuclear difusa. La carga positiva nuclear es equivalente a Ze, siendo e, la carga del electrón y  Z aproximadamente la mitad del peso atómico.</p>

             

            <p>Rutherford fue más allá y en diciembre de 1913 presenta la teoría de que la carga nuclear es una constante elemental que determina las propiedades químicas del átomo. Esta conjetura fue plenamente confirmada por su discípulo H. Moseley (1887 – 1915), quien demuestra experimentalmente la objetiva en el átomo de una magnitud elemental que aumenta una unidad al trasladar al elemento siguiente en la Tabla Periódica.  Esto puede detallarse si se admite que el número de orden del elemento en el sistema periódico, el número atómico, es equivalente a la carga nuclear.</p>

            <p>Durante este primer período la atención de la mayor parte de la vanguardia de los físicos teóricos se solidificaba en extender los argumentos cuánticos hechos por Planck; mientras, la edificación de un modelo para el núcleo atómico era un dilema relativamente relegado y frente al cual se levantaban enormes obstáculos teóricos y prácticos. Rutherford sugirió desde sus primeras investigaciones que muy probablemente el núcleo estaría constituido por las partículas alfa emitidas durante la desintegración radioactiva. Ya para entonces el propio Rutherford había cuidadosamente comprobado que las partículas alfa correspondían a núcleos del Helio, es decir, partículas de carga +2  y masa 4. Otra línea de pensamiento conducía a suponer que los electrones (partículas beta) emitidos durante la desintegración radioactiva eran lanzados desde el mismo núcleo.</p>

             

            <p>Frederick Soddy (1877 –1956), uno de los primeros y más sobresalientes radioquímicas, premio Nobel en 1921, al pretender ubicar el creciente número de productos de la desintegración radioactiva en la Tabla Periódica colocó los elementos que mostraban  propiedades químicas idénticas en la misma posición aunque presentaran diferentes masas atómicas. Al hacerlo estaba ignorando la ley de Mendeleev y modificando el propio concepto de elemento químico. Ahora surgía una nueva categoría para los átomos, el concepto de isótopos (del griego iso: único, topo: lugar). Poco después, el descubrimiento de Moseley apoyaría su decisión, al demostrar que la propiedad fundamental determinante de las propiedades químicas y de la propia identidad de los átomos era la carga nuclear.</p>

            <p>Con la Primera Guerra Mundial se levantaron obstáculos para el progreso de los estudios fundamentales recién iniciados, quedarían interrumpidos los intercambios científicos, detenidas las publicaciones, el campo de acción de las investigaciones avanzando a la práctica de la tecnología militar.</p>

            <p>Pero en Berlín una pareja de investigadores, Lise Meitner (1879 – 1968) y Otto Hahn (1878 – 1968), una física y un químico, ingresaban investigando sobre el aislamiento y la identificación de radioelementos y de productos de la desintegración radioactiva. Ante el alistamiento de Hahn en el ejército para llevar a cabo estudios vinculados con la naciente guerra química,  Meitner siguen las investigaciones y descubre en 1918 el protactinio.</p>

             

            <p>En 1919, Rutherford, que encabeza a partir de este año el laboratorio Cavendish en Cambridge, al estudiar el bombardeo con partículas alfa sobre átomos de nitrógeno, descubre la emisión de una nueva partícula, positiva, y evidentemente responsable de la carga nuclear del átomo, los protones. La existencia en el núcleo de partículas positivas y de los electrones emitidos como radiaciones beta, llevó a este relevante investigador a concebir una partícula que constituyese una formación neutral, un doblete comprendido como una unión estrecha de un protón y un electrón, el neutrón. Durante más de 10 años Rutherford y su principal asistente James Chadwick (1891 – 1974) intentaron en vano demostrar experimentalmente la existencia del neutrón.</p>

             

            <p>Las señales alentadoras vendrían de París, del laboratorio de los Joliot.  Jean Frederick (1900 – 1958) e Irene Joliot- Curie (1897 – 1956) reportaron en 1932 que al bombardear con partículas alfa, provenientes de una fuente de polonio, átomos de berilio se hacia una radiación de alto poder de penetración, nombrada únicamente "la radiación del berilio", que ellos asociaron a rayos γ. Pero Chadwick no compartió este supuesto y procedió a verificar que estas partículas eran los escurridizos neutrones. Chadwick fue acreditado para la Historia como el descubridor de los neutrones.</p>

             

            <p>La nueva oportunidad que se les presentó dos años más tarde a los Joliot fue esta vez convenientemente aprovechada. En 1928 Paúl Dirac (1902-1984) había predicho la existencia de la antipartícula del electrón, el positrón, que cuatro años después, experimentalmente descubre el físico norteamericano C. Anderson (1905 – 1991). Ellos encontraron que al bombardear aluminio con partículas alfa, la emisión de positrones continuaba después de retirar la fuente de plutonio, y además el blanco continuaba emitiendo conforme a la ley exponencial de la descomposición  de radionúclidos. Se había descubierto la radioactividad artificial.</p>

             

            <p>Rápidamente después del  hallazgo del neutrón, W.Heinseberg propone el modelo del núcleo del protón – neutrón. Conforme con este modelo los isótopos descubiertos por Soddy se distinguen sólo por el número de neutrones presentes en el núcleo. Este modelo se verificó minuciosamente y obtuvo una aprobación universal de la comunidad científica. Algunos cálculos preliminares estimaron la densidad del núcleo en ~ 1012 kg/m3, lo cual es un valor grande.</p>

            <p>Por otra parte, la presencia de los protones, partículas cargadas positivamente, confinadas a distancias del orden de las dimensiones del núcleo ~ 10-15 m implicaba la existencia de fuerzas de repulsión colombianas (de origen electrostático) gigantescas, que deberían ser compensadas por algún otro tipo de fuerza de atracción para mantenerlas no solo unidas, sino con una cohesión tal que su densidad tuviera los valores antes citados. Estas son las fuerzas nucleares, las cuales son de poco  alcance, muestran independencia respecto a la carga (ya que actúan por igual entre protones que entre neutrones) y presentan saturación dado que un nucleón solo interactúa con un número limitado de nucleones.</p>

            <p>La naturaleza de este nuevo tipo de fuerza, que se añadía a las conocidas anteriormente fuerzas gravitacionales y electromagnéticas, fue considerada como el tipo de intercambio, un nuevo concepto cuántico que involucra en la interacción entre nucleones el intercambio de una tercera partícula. En 1934 los científicos soviéticos Ígor Y. Tamm (1895-1971), premio Nobel de 1958 y el profesor Dimitri D. Ivanenko (1904- 1994) intentaron describir las fuerzas nucleares como fuerzas de intercambio en que las dos partículas interaccionan por medio de una tercera que intercambian continuamente. Ellos además comprobaron que no se podía explicar las fuerzas nucleares mediante el intercambio de ninguna de las partículas conocidas en aquel momento.</p>

            <p>En 1935 el físico japonés Hideki Yukawa dio una respuesta a este problema al suponer que ese intercambio se realiza mediante una nueva partícula: el mesón. En los dos años que siguieron se detectaron primero por Carl Anderson y luego por el británico Cecil Powell (1903 – 1969) partículas con similares características en los rayos cósmicos.</p>

             

            <p>Conjuntamente con el descubrimiento de las diferentes partículas constitutivas del núcleo fue surgiendo la necesaria pregunta de cuál era la estructura del mismo, o sea, de qué manera pudieran estar dispuestos los nucleones y así surgieron los primeros modelos del núcleo. Entre estos vale la pena citar el modelo de la gota líquida y el modelo de las capas.</p>

             

            <p>Cada uno de estos modelos se fundamenta en determinados resultados experimentales y logra explicar algunas de las características del núcleo. Por ejemplo, el modelo de la gota líquida se apoya en la analogía entre las fuerzas nucleares y las que se ejercen entre las moléculas de un líquido puesto que ambas presentan saturación. A partir del mismo se puede calcular la energía de enlace por nucleón teniendo en cuenta la energía volumétrica, la de tensión superficial y la de repulsión colombiana, la cual tiene un aspecto similar a la curva experimental. Sin embargo, no puede explicar los picos que tiene dicha curva para los núcleos de elementos tales como el He, C, O, Ca, etc.</p>

            <p>El modelo de la capas admite que el núcleo posee una estructura energética de niveles semejante a la estructura de capas electrónicas del átomo. En este sentido reproduce el esquema atómico para el núcleo. Este modelo explica satisfactoriamente la existencia de los números "mágicos", que corresponden al número total de nucleones de los núcleos más estables: 2, 8, 20, 50, 82 y 126. También justifica adecuadamente el valor de los espines nucleares, las grandes diferencias entre los períodos de semi-desintegración de los núcleos alfa-radiactivos, la radiación gamma, etc. No obstante, los valores de los momentos magnéticos muestran discrepancias con los valores experimentales.</p>

             

            <p>Otros modelos nucleares más desarrollados han sido concebidos de manera que tienen en cuenta elementos de los anteriores y en este sentido resulta su síntesis. Es preciso aclarar que aún en la actualidad no existe un modelo universal del núcleo capaz de explicar todas sus características.</p>

             

            <p>Sin embargo numerosas interrogantes quedaban en pie, entre otras flotaba la pregunta: ¿de dónde proceden los electrones resultantes de la desintegración radiactiva beta? Para responder a esta pregunta el eminente físico teórico suizo Wolfgang Pauli (1900 – 1978) supuso, en el propio 1932, que durante la desintegración beta junto con los electrones se emite otra partícula que acompaña la conversión del neutrón en un protón y un electrón y que porta la energía correspondiente al defecto de masa observado según la ecuación relativista de Einstein. Lo trascendente en la hipótesis de Pauli es que semejante partícula, necesaria para que el proceso obedeciera la ley de conservación y transformación de la energía,  no presentaba carga ni masa en reposo.</p>

             

            <p>Esta vez fueron 24 años, la espera necesaria para que la partícula postulada por Pauli y bautizada por Enrico Fermi (1901 - 1954) con el nombre de neutrino,  fuera  observada mediante experimentos indirectos conducidos de modo irrefutable por el físico norteamericano F. Reines (1918 - ). Con este descubrimiento se respaldaba la teoría desarrollada por Fermi sobre la desintegración beta y las llamadas fuerzas de interacción débil entre las partículas nucleares.</p>

             

            <p>Pero antes de esta espectacular verificación de la teoría, aún en la memorable y triste década de los 30, el propio Fermi y su grupo de la Universidad de Roma,  inició el camino hacia la fisión nuclear, considerando por el contrario que se dirigía hacia el descubrimiento de nuevos elementos más pesados.</p>

             

            <p>Algunas tecnologías derivadas de las teorías físicas y su aplicación.</p>

            <p>Cuando Roentgen descubre en 1895 los rayos de naturaleza entonces desconocida pero desde ya comprobada su alta capacidad de penetración pronto se aplica para obtener las primeras fotos de los huesos humanos. Su aplicación en Medicina encuentra una rápida difusión y en determinadas circunstancias históricas brilla en esta actividad la célebre Marie Curie. También con relativa rapidez se inaugura una época en que los rayos –X resultan útiles para analizar las sustancias cristalinas o los espectros de emisión de estas radiaciones por los elementos químicos permiten su identificación. Las páginas que siguen abordaran brevemente estos momentos.</p>

             

            <p>Con el propósito de apoyar la candidatura para una plaza vacante en la Academia de Ciencias del eminente físico Edouard Branly (1844-1940), que representaba los valores del conservadurismo francés, la prensa reaccionaria francesa no dudó en dañar la imagen de la insigne científica de origen polaco, Marie Curie. El daño se hizo y la candidatura de Marie fue derrotada en 1910 por dos votos. Un año después a su regreso del Congreso Solvay en Bruselas, debió enfrentar una nueva ronda de odio esta vez "acusada" de sostener relaciones con el destacado físico francés Paul Langevin (1872 – 1946). Poco después recibiría la información de la Academia Nobel de haber recibido un segundo Premio, esta vez en la disciplina de Química. De cualquier forma en los primeros meses de 1912, sufrió primero una fuerte depresión nerviosa y luego debió someterse una operación de los riñones. Sólo a fines de este año Marie retornó al laboratorio después de casi 14 meses de ausencia. El escándalo había finalizado y la Academia de Ciencias estaba dispuesta a darle la bienvenida a la mujer que había sido dos veces laureada con un premio Nobel.</p>

             

            <p>Pero pronto se pondría a prueba la estatura moral y el patriotismo verdadero que, durante el periodo de la guerra, iba a demostrar Marie por su nación de adopción. Por el otoño de 1914, cuando Alemania declaró la guerra a Francia, la construcción del Instituto de Radio había terminado pero la Curie no había trasladado aún su laboratorio para la nueva edificación. El trabajo del Instituto de Radio podría haber esperado por la restauración de la paz pero la Curie encontró formas de poner su conocimiento científico al servicio del país. En el Instituto de Radio, la Curie entrenó alrededor de 150 mujeres en la técnica de rayos – X que actuaron como asistentes en las unidades radiológicas móviles que fueron llevadas a las líneas del frente. Previamente había encabezado una campaña nacional para adaptar carros de aquellos tiempos como unidades radiológicas móviles que dieran una asistencia inmediata para el tratamiento de los heridos y fracturados en el campo de batalla. El uso de los rayos –X durante la guerra salvó las vidas de muchos heridos y redujo los sufrimientos de los que sufrieron fracturas de todo tipo.</p>

             

            <p>Cuando los servicios radiológicos ya estaban marchando establemente, Curie cambió su atención hacia el servicio de radioterapia. Comenzó entonces a usar una técnica desarrollada en Dublín para colectar radón, un gas radioactivo emitido continuamente por el radio. Trabajando sola y sin una protección adecuada Madame Curie pudo colectar el gas en ampolletas de vidrio selladas que eran así entregadas a los hospitales militares y civiles para que los médicos empleando agujas de platino lo inyectaran en la zona del cuerpo de los pacientes donde la radiación debía destruir el tejido enfermo. Se inauguraba la época de la radioterapia en la medicina.</p>

             

            <p>El rehallazgo de los rayos –X se creo cuando el físico alemán Max von Laue (1879 – 1960)  determina experimentalmente la longitud de onda de los rayos –X  al estudiar los espectros de difracción   que experimentan las sustancias cristalinas. Otros pioneros en el estudio de la estructura de los cristales mediante sus espectros de difracción de rayos –X  fueron los físicos británicos, padre e hijo, William Henry Bragg (1862-1942) y  William Lawrence Bragg (1890-1971). El primero fue profesor de Física de universidades inglesas y en el último tramo de su vida profesional ocupo la cátedra de Física de la Universidad de Londres. Su hijo le siguió los pasos en la investigación  y juntos desarrollaron trascendentales estudios sobre la estructura cristalina de importantes sustancias del mundo inorgánico demostrando la utilidad de la técnica como herramienta de investigación para confirmar las teorías cristalográficas. En reconocimiento a los logros cosechados compartieron padre e hijo el premio Nobel de Física de 1915. Nunca antes ni después se ha repetido este acontecimiento. William Lawrence fue sucesor en la Universidad de Manchester del físico nuclear Ernest Rutherford y luego funda en Cambridge, en 1938 el laboratorio de Biología Molecular que se destacará en los próximos años por los estudios fundamentales que desarrolla que cubren todo una época.</p>

            <p>Otro grande de las primeras décadas en el desarrollo de la espectroscopia de rayos –X fue el físico sueco, primer director del Instituto Nobel de Física Experimental, Karl Manne Georg Siegbahn (1886-1978). Siegbahn no se dedicó a la interpretación de los espectros de difracción sino al análisis de los rayos –X emitidos por los elementos químicos al ser bombardeados por electrones rápidos. En tales casos cada elemento ofrece un espectro de emisión característico. Los espectrómetros construidos por el propio Siegbahn permitían medir y registrar con alta precisión las longitudes de onda emitidas por cada elemento químico. Sus trabajos revelaron información sobre prácticamente todos los elementos químicos, desde el sodio hasta el uranio, lo que facilitó el análisis de sustancias desconocidas y encontraron aplicación en campos tan diversos como la física nuclear, la química, la astrofísica y la medicina.</p>

             

            <p>En artículos publicados en la Revista Journal of Applied Physics de 1963 y 1964 el físico sudafricano, naturalizado en EU, Allan M. Cormack (1924 -1998) expusó los principios de una nueva técnica que aplicaba un barrido multidireccional de rayos –X sobre el paciente para luego reconstruir las imágenes de sus órganos con mayor resolución que las técnicas convencionales. Estos trabajos no llamaron la atención de la comunidad de radiólogos hasta que en 1967 el ingeniero  electrónico británico Godfrey N. Hounsfield (1919-2004) sin conocerlos desarrolló  el escáner un equipo que bajo el mismo principio propuesto por Cormack iba a representar una de las más importantes invenciones médicas del siglo XX: la tomografía axial computarizada (TAC). Los escáneres se empezaron a utilizar  en la década de 1970 y en la actualidad se emplean en muchos países, sobre todo para diagnosticar el cáncer. La tomografía permite un diagnóstico más preciso al obtener imágenes tridimensionales con una resolución mucho mayor.   En 1979 Cormack y Hounsfield compartieron el Premio Nobel de Fisiología y Medicina.</p>
        </div>
      </div>
    </div>
    <div class="mdl-cell mdl-cell--2-col"></div>
</div>